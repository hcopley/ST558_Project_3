---
title: "`r paste('ST 558 Project 3 \n Education Level: ', params$title)`"

author: "Heather Copley & Andy Johnson"
date: "`r Sys.Date()`"
output: github_document
dfpring: paged
params:
    education: 1
    title: "Never attended school or only kindergarten"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(tidyverse)
library(summarytools)
library(caret)
library(rattle)
```

## Introduction

This report automates the construction of exploratory analyses and prediction models using data from the 2015 [Behavioral Risk Factor Surveillance System](https://www.cdc.gov/brfss/index.html) (BRFSS). The BRFSS is a long-running telephone survey in the United States that collects data on health-related issues, behaviors, and service utilization. The primary outcome of interest in these analyses is if a respondent reports being diabetic or prediabetic. Diabetes is an chronic endocrine disorder in which the body is unable to properly regulate blood glucose levels. [Approximately one in nine U.S. adults has diabetes](https://diabetes.org/about-diabetes/statistics/about-diabetes), and this prevalence has steadily increased over the last few decades. [Prediabetes](https://www.mayoclinic.org/diseases-conditions/prediabetes/symptoms-causes/syc-20355278) is a transitional state of chronically heightened blood glucose levels that is just below the clinical threshold for diabetes. Early detection and treatment of prediabetes can restore blood glucose to normal levels, and the appropriate clinical management of diabetes can reduce long-term negative health outcomes and healthcare costs due to the disease. For these reasons, it is important to identify and understand the various personal characteristics that are associated with having diabetes.

In this report, we use a subset of BRFSS data to explore a variety of factors associated with having diabetes or prediabetes, and build prediction models to estimate the probability of a survey respondent having diabetes. This report is part of a larger series of reports, each one focusing on a subset of BRFSS survey respondent defined by having a common level of education. The analyses below include descriptive statistical summaries, exploratory visualizations/graphs, and the creation and evaluation of prediction models for diabetic status for individuals with an education level of `r str_to_lower(params$title)`.

The variables used in these analyses include individually-reported characteristics such as:

-   a history of chronic comorbidities (hypertension, hypercholesterolemia, heart disease, depression),
-   a history of acute health events (heart attacks, stroke),
-   other relevant physical characteristics (BMI, age, sex),
-   behavorial factors that influence health (smoking, diet, physical activity), and
-   socioeconomic factors (income, health insurance coverage, education levels).

## Exploratory Data Analysis

The codebook for the variables included in this analysis can be found here:  

[Behavioral Risk Factor Surveillance System Codebook](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://www.cdc.gov/brfss/annual_data/2015/pdf/codebook15_llcp.pdf)

We used these definitions to properly format and describe the data. 

```{r}
#read in data 
#filter to parameter education
#and format factors
diabetes_data <- read_csv('diabetes_binary_health_indicators_BRFSS2015.csv') %>%
    filter(Education == params$education) %>%
    mutate_at(vars(Diabetes_binary,
                   HighBP,
                   HighChol,
                   CholCheck,
                   Smoker,
                   Stroke,
                   HeartDiseaseorAttack,
                   PhysActivity,
                   Fruits,
                   Veggies,
                   HvyAlcoholConsump, 
                   AnyHealthcare,
                   NoDocbcCost,
                   DiffWalk
                  ),
              ~factor(., 
                      levels = c(0,1), 
                      labels = c("no", "yes"))) %>%
    mutate(GenHlth = factor(GenHlth, 
                            levels = c(1:5), 
                            labels = c("excellent", "very good", "good", "fair", "poor")),
           Sex = factor(Sex, 
                        levels = c(0,1), 
                            labels = c("female", "male")),
           Age = factor(Age,
                        levels = c(1:13),
                        labels = c("18-24",
                                   "25-29",
                                   "30-34",
                                   "35-39",
                                   "40-44",
                                   "45-49",
                                   "50-54",
                                   "55-59",
                                   "60-64",
                                   "65-69",
                                   "70-74",
                                   "75-79",
                                   "80-99")),
           Education = factor(Education,
                        levels = c(1:6),
                        labels = c("Never attended school or only kindergarten",
                                    "Grades 1 through 8 (Elementary)",
                                    "Grades 9 through 11 (Some high school)",
                                    "Grade 12 or GED (High school graduate)",
                                    "College 1 year to 3 years (Some college or technical school)",
                                    "College 4 years or more (College graduate)")),
          Income = factor(Income,
                          levels = c(1:8), 
                            labels = c("Less than $10,000",
                                        "$10,000 to less than $15,000",
                                        "$15,000 to less than $20,000",
                                        "$20,000 to less than $25,000",
                                        "$25,000 to less than $35,000",
                                        "$35,000 to less than $50,000",
                                        "$50,000 to less than $75,000",
                                        "$75,000 or more"))
    )

#create a vector of column descriptions/labels
descriptions <- c("Diabetes or prediabetes?"
            ,"High Blood Pressure?"
            ,"High Cholesterol?"
            ,"Cholesterol check in the past 5 years?"
            ,"Body Mass Index"
            ,"Have you smoked at least 100 cigarettes in your entire life? [Note: 5 packs = 100 cigarettes]"
            ,"(Ever told) you had a stroke?"
            ,"Coronary heart disease (CHD) or myocardial infarction (MI)?"
            ,"Physical activity in past 30 days - not including job?"
            ,"Consume Fruit 1 or more times per day?"
            ,"Consume Vegetables 1 or more times per day?"
            ,"Heavy Alcohol Consumption (adult men >=14 drinks per week and adult women>=7 drinks per week)"
            ,"Have any kind of health care coverage, including health insurance, prepaid plans such as HMO, etc.?"
            ,"Was there a time in the past 12 months when you needed to see a doctor but could not because of cost?" 
            ,"Would you say that in general your health is:" 
            ,"Now thinking about your mental health, which includes stress, depression, and problems with emotions, for how many days during the past 30 days was your mental health not good?"
            ,"Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?"
            ,"Do you have serious difficulty walking or climbing stairs?"
            ,"Sex"
            ,"Age Group"
            ,"What is the highest grade or year of school you completed?" 
            ,"Is your annual household income from all sources:") 


#add variable descriptions as labels
label(diabetes_data) <- descriptions

subtitle <- diabetes_data$Education %>%
    droplevels() %>%
    unique() %>%
    as.character() %>%
    paste0('Education Level: ', .)

```

### Summary Statistics

Univariate summary statistics of each variable can be seen below:

```{r}
#show univariate summary statistics
print(dfSummary(diabetes_data
                ,varnumbers = FALSE
                ,valid.col = FALSE
                ,graph.magnif = .8
                ),
      method = 'render',
      headings = FALSE,
      bootstrap.css = FALSE
      )

```

### Contingency Tables

#### High Blood Pressure

```{r}

#create contingency table of High Blood Pressure and Diabetes
with(diabetes_data,
    ctable( x = HighBP, 
            y = Diabetes_binary,
            chisq = TRUE)) %>%
    print(method = 'render')

```

#### High Cholesterol

```{r}

#create contingency table of High Cholesterol Pressure and Diabetes
with(diabetes_data,
    ctable( x = HighChol, 
            y = Diabetes_binary,
            chisq = TRUE)) %>%
    print(method = 'render')

```

#### Deferred care because of cost by Coverage Status

```{r}

#create contingency table of Deferred care because of cost versus Coverage
with(diabetes_data,
    ctable( x = AnyHealthcare, 
            y = NoDocbcCost,
            chisq = TRUE)) %>%
    print(method = 'render')

```

### Plots

#### BMI

We can see from the boxplots below that Individuals with diabetes or prediabetes have a higher BMI on average. 

```{r}
#BMI boxplots

ggplot(data = diabetes_data, aes(y = BMI, fill = Diabetes_binary)) + 
    geom_boxplot()

```

#### Health Status (Past 30 Days)

Individuals that do not have diabetes or prediabetes were more likely to report having 0 days in the past 30 days of not good mental or physical health. 

```{r}
#reformat the not healthy days variables into long 
health_days <- diabetes_data %>%
    select(Diabetes_binary, MentHlth, PhysHlth) %>%
    pivot_longer(cols = c(MentHlth, PhysHlth)) %>%
    mutate(name = if_else(name == 'MentHlth', 'Days of Not Good Mental Health (Past 30 days)', 'Days of Not Good Physical Health (Past 30 days)')) %>%
    mutate(Diabetes = if_else(Diabetes_binary == 'yes', 'Diabetes or Prediabetes', 'No Diabetes'))
    
#plot not good healthy days by diabetes
ggplot(data = health_days, aes(x = value, fill = Diabetes)) + 
    geom_histogram(bins =5, alpha = .5, position = "dodge", show.legend = FALSE) + 
    facet_wrap(.~name + Diabetes) 

```

#### Income

There are more individuals in the dataset who reported income in the lower ranges, however those individuals with a household income below 15k were disproportionately more likely than higher income individuals to have diabetes or prediabetes.

```{r}

ggplot(data = diabetes_data, aes(y = Income, fill = Diabetes_binary)) +
    geom_bar(position = 'dodge')
    

```



#### BMI by Vegetables in Diet, across Sex

```{r}

ggplot(data = diabetes_data, aes(x=BMI, y=Veggies, fill=Sex )) + 
  geom_boxplot()

```


#### BMI by Smoking Status, across Sex

```{r}

ggplot(data = diabetes_data, aes(x=BMI, y=Smoker, fill=Sex )) + 
  geom_boxplot()

```



## Modeling

### Data Partition

We split the data into training and testing sets with 70% in the training set and 30% in the testing set.

```{r}

#split data into 70/20 train/test
set.seed(42)
train_index <- createDataPartition(diabetes_data$Diabetes_binary, p = .7, list = FALSE) 

diabetes_train <- diabetes_data[train_index, ]
diabetes_test <- diabetes_data[-train_index, ]


```

### Data Preprocessing

We applied centering and scaling to our training and test data.

```{r}

#set up center and scale from the training set
pre_process_values <- preProcess(diabetes_train, method = c("center", "scale"))
#apply center and scaling to both the training and test set
train_preprocessed <- predict(pre_process_values, diabetes_train)
test_preprocessed <- predict(pre_process_values, diabetes_test)

```

### Log Loss [Heather]

In binary classification problems log loss is used to measure performance by comparing the predicted values to the actual values. Log loss increases as the predicted probability diverges further away from the actual. Log loss is a value between 0 and 1 with a perfect model having a log loss of 0.

We can calculate log loss as follows:

$$log loss = \frac{1}{N}\sum[y_i*log(p_i)+(1-y_i)*log(1-p_i)]$$ Where:

-   $N$ = the number of predictions
-   $y_i$ = the actual outcome of instance $i$
-   $p_i$ = the probability that the model predicts for instance $i$

Logloss has some advantages over other measures of performance. It works well with imbalanced classes in the outcome variable where measures like accuracy can be misleading.

We set our train control metric to logloss

```{r}

train_control <- trainControl(method = 'cv',
                              number = 5,
                              classProbs = TRUE,
                              summaryFunction = mnLogLoss)

```

### Logistic Regression [Heather]

[Logistic regression](https://simple.wikipedia.org/wiki/Logistic_regression) is a generalized linear model appropriate to binary outcome data. The predictor variables may be continuous or categorical. It models the probability of success of the outcome class using the logistic function

$$P(success) = \frac{e^{\beta_0 + B_1x_1 + B_2x_2 +... + B_nx_n}}{1 + e^{\beta_0 + B_1x_1 + B_2x_2 +... + B_nx_n}}$$

In logistic regression the logit function links the probability to a linear combination of the parameters: 

$$logit(p) = log(\frac{p}{1-p})$$

It can be shown that: 

$$log(\frac{p}{1-p}) = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n + \epsilon$$


Aside from the dependent variable being binary Logistic regression does have a few other key assumptions: 

* Independence of errors
* The predictor variables are linearly related to the log odds/logit
* Absence of Multicollinearity
* Absence of strongly influential outliers

For this project we fit three different logistic regression models

```{r}

#note there are no tuning parameters available for this model
set.seed(42)
log_reg_1 <- train(Diabetes_binary ~ ., 
                   data = train_preprocessed, 
                   method = 'glm', family = 'binomial',
                   metric="logLoss",
                   trControl = train_control)

log_reg_2 <- train(Diabetes_binary ~ BMI + HighBP + HighChol + MentHlth,
                   data = train_preprocessed, 
                   method = 'glm', family = 'binomial',
                   metric="logLoss",
                   trControl = train_control)


log_reg_3 <- train(Diabetes_binary ~ HighBP + HeartDiseaseorAttack + AnyHealthcare,
                   data = train_preprocessed, 
                   method = 'glm', family = 'binomial',
                   metric="logLoss",
                   trControl = train_control)


```

### LASSO [Andy]

The [LASSO (Least Absolute Shrinkage and Selection Operator)](https://en.wikipedia.org/wiki/Lasso_(statistics)) is a regression modeling technique that adds a penalty term to the loss/cost function. The penalty term (seen below) is the sum of the absolute values of the estimated beta coefficients (also called the L1-norm or "Manhattan" distance), multiplied by a hyperparameter $\lambda$. This penalty term is added to the appropriate loss function, and the model estimate procedure attempts to minimize the entire quantity.

$$lasso: log loss + penalty = \frac{1}{N}\sum[y_i*log(p_i)+(1-y_i)*log(1-p_i)] + \lambda\sum||\beta||$$

The first quantity (the original loss function) is minimized by choosing values for $\beta$ that are not equal to zero (assuming there is any "signal" at all present in the input variables). However, choosing non-zero values for $\beta$ will then increase the value of the penalty term. So in minimizing the entire penalized loss function, the two parts are at odds with each other: decreasing one part necessarily increases the other, and vice versa. The net effect is that the values of $\beta$ chosen in this procedure are shrunken (biased to be closer to zero) versus those found in the usual un-penalized approach. The $\lambda$ hyperparameter adjusts the relative impact of the penalty on the overall loss function. You can think of $\lambda$ as the "exchange rate that governs the amount of non-zero coefficients you can buy".

The end result is that you intentionally introduce bias into the coefficients in order to reduce model variance. Models training using penalization are simpler and less likely to be overfit. An interesting side-effect of using the absolute value function in the penalty term is that the model estimates for very small coefficients often get shrunk directly to zero. In this manner, the LASSO performs variable selection.

In summary, there are two reasons why you would want to use the LASSO over standard logistic regression:

1.  If you have many input variables (perhaps with some collinearity issues), and you want an automated variable selection routine that performs better than stepwise/forward/backward selection, and
2.  You want a final model that is not over-fit so it can predict well on data it hasn't yet seen (and you prefer to stay within a linear model paradigm).

```{r}

set.seed(42)
lasso <- train(Diabetes_binary ~ ., 
               data = train_preprocessed, 
               method = 'glmnet', family = 'binomial',
               metric="logLoss",
               trControl = train_control,
               tuneGrid = expand.grid(alpha = 1, lambda = seq(0,1,0.005)))


# lasso$bestTune
# plot(lasso$finalModel, "lambda")
# abline(v = log(lasso$bestTune$lambda), lty = 2)
# text(x=log(lasso$bestTune$lambda), y=0, labels="Lambda for\nmin CV-LogLoss", cex=0.75, adj=0.55, srt=90)
```

### Classification Tree [Andy]

Classification tree models attempt to recursively partition the input feature space into local areas that maximize the homogeneity of the outcome instances contained within them. Or said more simply, classification trees find combinations of input variable values that tend to associate with one of the output variable's classes. Predictions made with classification tree models are very easy to express in simple terms, as they are just intersections of logical conditions. For example:

*if (class = senior) and (credit_hours \> 120) and (parking_fines_outstanding = 0), then ready_to_graduate = yes.*

For this reason, classification trees are useful for prediction models that must be directly inspected. They also innately handle interactions between variables and non-linearities by simply having an interacting effect appear in multiple "levels" of a branch of the tree. Unfortunately, individual classification trees are considered to be weak learners that are high-bias (typically underfitted). To remedy this, we often create large ensembles of classification trees, each one "grown" slightly different from each other through the use of resampling rows of the training data (bagging) and limiting the selection set at each split (random forests). Classification trees are also a poor choice if we strongly believe the underlying mechanism that generated the data was a linear combination of inputs (like a regression model).

```{r, fig.asp=1}


set.seed(42)
# train a single classification tree
tree <- train(Diabetes_binary ~ ., 
                  data = train_preprocessed, 
                  method = "rpart",
                  metric="logLoss",
                  trControl = train_control,
                  tuneGrid = expand.grid(cp=seq(0,1,0.01)))

tree$bestTune
tree$finalModel

#if there is more than a root node in the final model then create a tree plot:
if (length(tree$finalModel$splits) > 0) {
fancyRpartPlot(tree$finalModel)

} 
```

### Random Forest [Heather]

Random forest is an ensemble method which uses many trees (in our case classification trees) to predict the outcome. These trees are each fit to a different bootstrap sample of the original training set. Each bootstrap sample is sampled with replacement and has the same number of observations as the training set. Each model fit has a random selection of the variables is chosen. 


```{r, eval = F}
set.seed(42)
random_forest <- train(Diabetes_binary ~ .,
            data = train_preprocessed,
            method = "ranger",
            trControl = train_control,
            metric="logLoss",
            tuneGrid = data.frame(mtry = 1:ncol(train_preprocessed)-1, min.node.size = 1, splitrule = 'gini')
            )

#get the top n variables by importance where n = the best tune (best # of predictors chosen)
variable_importance <- varImp(random_forest)$importance %>% arrange(-Overall) %>% head(random_forest$bestTune$mtry) %>%
    rownames_to_column('variable') %>%
    mutate(variable = fct_reorder(variable, Overall))

#create lollipop plot of the top n variables by variable importance
ggplot(variable_importance, aes(y = variable, x = Overall)) + 
    geom_point() + 
    geom_segment(aes(y=variable, yend=variable, x=0, xend=Overall)) +
    xlab('Overall Importance')

```

In our model we tuned the variable mtry which is the number of predictor variables to include in the models. The best tune turned out to be `r random_forest$bestTune$mtry`. The top `r random_forest$bestTune$mtry` variables by importance are shown above


### Ridge Regression [Andy]

Ridge regression can be thought of as a sibling to the LASSO: they vary only by the function/norm used in the penalty function. As seen below, the penalized loss function for ridge regression replaces the absolute value in the penalty term with a squared exponent:

$$ridge: log loss + penalty = \frac{1}{N}\sum[y_i*log(p_i)+(1-y_i)*log(1-p_i)] + \lambda\sum\beta^2$$

Instead of summing the absolute values of the $\beta$ coefficients, we sum their squares. This still enforces a penalty that grows along with the coefficients, but it does not force smaller estimates to zero like the LASSO does. Because of this, ridge regression does not perform variable selection.

The reason you might want ridge regression over standard logistic regression and the LASSO is that you have many input variabes (perhaps with collinearity issues), and you want good out-of-sample predictive performance but don't want variable selection. In cases where you might have real-but-small effects for some inputs, the LASSO would shrink them to 0, while ridge would just shrink them smaller (but not necessarily to 0).

```{r}

set.seed(42)
ridge <- train(Diabetes_binary ~ ., 
               data = train_preprocessed, 
               method = 'glmnet', family = 'binomial',
               metric="logLoss",
               trControl = train_control,
               tuneGrid = expand.grid(alpha = 0, lambda = seq(0,1,0.005)))

# ridge$bestTune
# plot(ridge$finalModel, "lambda")
# abline(v = log(ridge$bestTune$lambda), lty = 2)
# text(x=log(ridge$bestTune$lambda), y=0, labels="Lambda for\nmin CV-LogLoss", cex=0.75, adj=0.55, srt=90)
```

### Elastic Net [Heather]

[Elastic net](https://en.wikipedia.org/wiki/Elastic_net_regularization) combines the features of both Lasso and Ridge regression. It does so by incorporating both of their penalty terms $L_1$ (Lasso) and $L_2$ (Ridge) into the loss function of the regression model. A weighted sum of the two penalties is taken as the regularization term. The penalty term is a linear combination of $L_1$ and $L_2$ controlled by a mixing term $\alpha$ when $\alpha = 1$ the penalty is purely L_1 (Lasso) and when $\alpha = 0$ the penalty is purely L_2 (Ridge). 

Elastic net overcomes some of the limitations of Lasso: 

* In the case of a large number of predictors with fewer observations (large p, small n) lasso will select n variables at most. 
* In the case of highly corrolated variables Lasso tends to select one variable and ignore the others. 

Elastic net overcomes these limitations by adding a quadratic to the penalty $||\beta||^2$. When used alone this penalty is ridge regression. 

The estimates of elastic net are modeled as: 

$$\hat\beta = \underset{\beta}argmin(||y=X\beta||^2 +\lambda _{2}\|\beta \|^{2}+\lambda _{1}\|\beta \|_{1} )$$

```{r}

lambda <- seq(0, 3, 0.1)
alpha <- seq(0, 1, 0.1)

grid <- expand.grid(alpha = alpha, lambda = lambda)


set.seed(42)
elastic_net <- train(Diabetes_binary ~ .,
                     data = train_preprocessed,
                     method = "glmnet",
                     metric="logLoss",
                     trControl = train_control,
                     tuneGrid = grid
)



elastic_net$bestTune

```

## Final Model Selection

```{r compare_logloss_on_test}
# stack model objects into a list
models <- list(logistic_regression_1 = log_reg_1,
               logistic_regression_2 = log_reg_2,
               logistic_regression_3 = log_reg_3,
               tree = tree,
               #random_forest = random_forest,
               ridge = ridge,
               lasso = lasso,
               elastic_net = elastic_net)

# create function to extract logloss on test dataset
get_logloss_on_test <- function(model){
  test_pred <- predict(model, newdata = test_preprocessed, type="prob")
  test_set <- data.frame(obs = test_preprocessed$Diabetes_binary, 
                         yes = test_pred[,2],
                         no = test_pred[,1])
  ll <- mnLogLoss(test_set, lev = levels(test_set$obs))
  
  return( ll )
}

# use function to get test set accuracy numbers
perf <- sapply(models, get_logloss_on_test)
perf

#get the best performing model which minimizes logloss
top_model <- names(which.min(perf)) %>%
    str_replace_all('.logLoss|_', ' ') %>%
    str_squish %>%
    str_to_title

```

The model with the lowest log loss is the `r top_model` model with a log loss of = `r round(min(perf),3)`.
