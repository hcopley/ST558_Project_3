---
title: "`r paste('ST 558 Project 3 \n Education Level: ', params$title)`"

author: "Heather Copley & Andy Johnson"
date: "`r Sys.Date()`"
output: github_document
dfpring: paged
params:
    education: 1
    title: "Never attended school or only kindergarten"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(tidyverse)
library(summarytools)
library(DT)
library(caret)
library(rattle)
```

## Introduction

This report automates the construction of exploratory analyses and prediction models using data from the 2015 [Behavioral Risk Factor Surveillance System](https://www.cdc.gov/brfss/index.html) (BRFSS). The BRFSS is a long-running telephone survey in the United States that collects data on health-related issues, behaviors, and service utilization. The primary outcome of interest in these analyses is if a respondent reports being diabetic or prediabetic. Diabetes is an chronic endocrine disorder in which the body is unable to properly regulate blood glucose levels. [Approximately one in nine U.S. adults has diabetes](https://diabetes.org/about-diabetes/statistics/about-diabetes), and this prevalence has steadily increased over the last few decades. [Prediabetes](https://www.mayoclinic.org/diseases-conditions/prediabetes/symptoms-causes/syc-20355278) is a transitional state of chronically heightened blood glucose levels that is just below the clinical threshold for diabetes. Early detection and treatment of prediabetes can restore blood glucose to normal levels, and the appropriate clinical management of diabetes can reduce long-term negative health outcomes and healthcare costs due to the disease. For these reasons, it is important to identify and understand the various personal characteristics that are associated with having diabetes.

In this report, we use a subset of BRFSS data to explore a variety of factors associated with having diabetes or prediabetes, and build prediction models to estimate the probability of a survey respondent having diabetes. This report is part of a larger series of reports, each one focusing on a subset of BRFSS survey respondent defined by having a common level of education. The analyses below include descriptive statistical summaries, exploratory visualizations/graphs, and the creation and evaluation of prediction models for diabetic status.

The variables used in these analyses include individually-reported characteristics such as:

-   a history of chronic comorbidities (hypertension, hypercholesterolemia, heart disease, depression),
-   a history of acute health events (heart attacks, stroke),
-   other relevant physical characteristics (BMI, age, sex),
-   behavorial factors that influence health (smoking, diet, physical activity), and
-   socioeconomic factors (income, health insurance coverage, education levels).

## Exploratory Data Analysis

[Behavioral Risk Factor Surveillance System Codebook](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://www.cdc.gov/brfss/annual_data/2015/pdf/codebook15_llcp.pdf)

```{r}
#read in data 
#filter to parameter education
#and format factors
diabetes_data <- read_csv('diabetes_binary_health_indicators_BRFSS2015.csv') %>%
    filter(Education == params$education) %>%
    mutate_at(vars(Diabetes_binary,
                   HighBP,
                   HighChol,
                   CholCheck,
                   Smoker,
                   Stroke,
                   HeartDiseaseorAttack,
                   PhysActivity,
                   Fruits,
                   Veggies,
                   HvyAlcoholConsump, 
                   AnyHealthcare,
                   NoDocbcCost,
                   DiffWalk
                  ),
              ~factor(., 
                      levels = c(0,1), 
                      labels = c("no", "yes"))) %>%
    mutate(GenHlth = factor(GenHlth, 
                            levels = c(1:5), 
                            labels = c("excellent", "very good", "good", "fair", "poor")),
           Sex = factor(Sex, 
                        levels = c(0,1), 
                            labels = c("female", "male")),
           Age = factor(Age,
                        levels = c(1:13),
                        labels = c("18-24",
                                   "25-29",
                                   "30-34",
                                   "35-39",
                                   "40-44",
                                   "45-49",
                                   "50-54",
                                   "55-59",
                                   "60-64",
                                   "65-69",
                                   "70-74",
                                   "75-79",
                                   "80-99")),
           Education = factor(Education,
                        levels = c(1:6),
                        labels = c("Never attended school or only kindergarten",
                                    "Grades 1 through 8 (Elementary)",
                                    "Grades 9 through 11 (Some high school)",
                                    "Grade 12 or GED (High school graduate)",
                                    "College 1 year to 3 years (Some college or technical school)",
                                    "College 4 years or more (College graduate)")),
          Income = factor(Income,
                          levels = c(1:8), 
                            labels = c("Less than $10,000",
                                        "$10,000 to less than $15,000",
                                        "$15,000 to less than $20,000",
                                        "$20,000 to less than $25,000",
                                        "$25,000 to less than $35,000",
                                        "$35,000 to less than $50,000",
                                        "$50,000 to less than $75,000",
                                        "$75,000 or more"))
    )

#create a vector of column descriptions/labels
descriptions <- c("Diabetes or prediabetes?"
            ,"High Blood Pressure?"
            ,"High Cholesterol?"
            ,"Cholesterol check in the past 5 years?"
            ,"Body Mass Index"
            ,"Have you smoked at least 100 cigarettes in your entire life? [Note: 5 packs = 100 cigarettes]"
            ,"(Ever told) you had a stroke?"
            ,"Coronary heart disease (CHD) or myocardial infarction (MI)?"
            ,"Physical activity in past 30 days - not including job?"
            ,"Consume Fruit 1 or more times per day?"
            ,"Consume Vegetables 1 or more times per day?"
            ,"Heavy Alcohol Consumption (adult men >=14 drinks per week and adult women>=7 drinks per week)"
            ,"Have any kind of health care coverage, including health insurance, prepaid plans such as HMO, etc.?"
            ,"Was there a time in the past 12 months when you needed to see a doctor but could not because of cost?" 
            ,"Would you say that in general your health is:" 
            ,"Now thinking about your mental health, which includes stress, depression, and problems with emotions, for how many days during the past 30 days was your mental health not good?"
            ,"Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?"
            ,"Do you have serious difficulty walking or climbing stairs?"
            ,"Sex"
            ,"Age Group"
            ,"What is the highest grade or year of school you completed?" 
            ,"Is your annual household income from all sources:") 


#add variable descriptions as labels
label(diabetes_data) <- descriptions

subtitle <- diabetes_data$Education %>%
    droplevels() %>%
    unique() %>%
    as.character() %>%
    paste0('Education Level: ', .)

```

### Summary Statistics

```{r}
#show univariate summary statistics
print(dfSummary(diabetes_data
                ,varnumbers = FALSE
                ,valid.col = FALSE
                ,graph.magnif = .8
                ),
      method = 'render',
      headings = FALSE,
      bootstrap.css = FALSE
      )

```

### Contingency Tables

#### High Blood Pressure

```{r}

#create contingency table of High Blood Pressure and Diabetes
with(diabetes_data,
    ctable( x = HighBP, 
            y = Diabetes_binary,
            chisq = TRUE)) %>%
    print(method = 'render')

```

#### High Cholesterol

```{r}

#create contingency table of High Cholesterol Pressure and Diabetes
with(diabetes_data,
    ctable( x = HighChol, 
            y = Diabetes_binary,
            chisq = TRUE)) %>%
    print(method = 'render')

```

#### Deferred care because of cost by Coverage Status

```{r}

#create contingency table of Deferred care because of cost versus Coverage
with(diabetes_data,
    ctable( x = AnyHealthcare, 
            y = NoDocbcCost,
            chisq = TRUE)) %>%
    print(method = 'render')

```

### Plots

#### BMI

```{r}
#BMI boxplots

ggplot(data = diabetes_data, aes(y = BMI, fill = Diabetes_binary)) + 
    geom_boxplot()

```

#### Health Status (Past 30 Days)

```{r}
#reformat the not healthy days variables into long 
health_days <- diabetes_data %>%
    select(Diabetes_binary, MentHlth, PhysHlth) %>%
    pivot_longer(cols = c(MentHlth, PhysHlth)) %>%
    mutate(name = if_else(name == 'MentHlth', 'Days of Not Good Mental Health (Past 30 days)', 'Days of Not Good Physical Health (Past 30 days)')) %>%
    mutate(Diabetes = if_else(Diabetes_binary == 'yes', 'Diabetes or Prediabetes', 'No Diabetes'))
    
#plot not good healthy days by diabetes
ggplot(data = health_days, aes(x = value, fill = Diabetes)) + 
    geom_histogram(bins =5, alpha = .5, position = "dodge", show.legend = FALSE) + 
    facet_wrap(.~name + Diabetes) 

```

#### Income

```{r}

ggplot(data = diabetes_data, aes(y = Income, fill = Diabetes_binary)) +
    geom_bar(position = 'dodge')

```

## Modeling

### Data Partition

We split the data into training and testing sets with 70% in the training set and 30% in the testing set.

```{r}

#split data into 70/20 train/test
set.seed(42)
train_index <- createDataPartition(diabetes_data$Diabetes_binary, p = .7, list = FALSE) 

diabetes_train <- diabetes_data[train_index, ]
diabetes_test <- diabetes_data[-train_index, ]


```

### Data Preprocessing

We applied centering and scaling to our training and test data.

```{r}

#set up center and scale from the training set
pre_process_values <- preProcess(diabetes_train, method = c("center", "scale"))
#apply center and scaling to both the training and test set
train_preprocessed <- predict(pre_process_values, diabetes_train)
test_preprocessed <- predict(pre_process_values, diabetes_test)

```

### Log Loss [Heather]

In binary classification problems log loss is used to measure performance by comparing the predicted values to the actual values. Log loss increases as the predicted probability diverges further away from the actual. Log loss is a value between 0 and 1 with a perfect model having a log loss of 0.

We can calculate log loss as follows:

$$log loss = \frac{1}{N}\sum[y_i*log(p_i)+(1-y_i)*log(1-p_i)]$$ Where:

-   $N$ = the number of predictions
-   $y_i$ = the actual outcome of instance $i$
-   $p_i$ = the probability that the model predicts for instance $i$

Logloss has some advantages over other measures of performance. It works well with imbalanced classes in the outcome variable where measures like accuracy can be misleading.

We set our train control metric to logloss

```{r}

train_control <- trainControl(method = 'cv',
                              number = 5,
                              classProbs = TRUE,
                              summaryFunction = mnLogLoss)

```

### Logistic Regression [Heather]

```{r}

#note there are no tuning parameters available for this model
set.seed(42)
log_reg_1 <- train(Diabetes_binary ~ ., 
                   data = train_preprocessed, 
                   method = 'glm', family = 'binomial',
                   trControl = train_control)

log_reg_2 <- train(Diabetes_binary ~ BMI + HighBP + HighChol + MentHlth,
                   data = train_preprocessed, 
                   method = 'glm', family = 'binomial',
                   trControl = train_control)

log_reg_3 <- train(Diabetes_binary ~ HighBP + HeartDiseaseorAttack + AnyHealthcare,
                   data = train_preprocessed, 
                   method = 'glm', family = 'binomial',
                   trControl = train_control)

log_reg_1$results
log_reg_2$results
log_reg_3$results


```

### LASSO [Andy]

The [LASSO (Least Absolute Shrinkage and Selection Operator)](https://en.wikipedia.org/wiki/Lasso_(statistics)) is a regression modeling technique that adds a penalty term to the loss/cost function. The penalty term (seen below) is the sum of the absolute values of the estimated beta coefficients (also called the L1-norm or "Manhattan" distance), multiplied by a hyperparameter $$\lambda$$. This penalty term is added to the appropriate loss function, and the model estimate procedure attempts to minimize the entire quantity.

$$log loss + penalty = \frac{1}{N}\sum[y_i*log(p_i)+(1-y_i)*log(1-p_i)] + \lambda\sum||\beta||$$

The first quantity (the original loss function) is minimized by choosing values for $$\beta$$ that are not equal to zero (assuming there is any signal at all). However, choosing non-zero values for $$\beta$$ will then increase the value of the penalty term. So in minimizing the entire penalized loss function, the two parts are "at odds": decreasing one part increases the other, and vice versa. The net effect is that the values of $$\beta$$ chosen in this procedure are shrunken (closer to zero) than they would be in an unpenalized approach.

### Classification Tree [Andy]

```{r, fig.asp=1}

set.seed(42)
# train a single classification tree
tree_fit <- train(Diabetes_binary ~ ., 
                  data = train_preprocessed, 
                  method = "rpart",
                  trControl = train_control,
                  tuneGrid = expand.grid(cp=seq(0,1,0.01)))

tree_fit$bestTune
tree_fit$finalModel
fancyRpartPlot(tree_fit$finalModel)
```

### Random Forest [Heather]

```{r}

set.seed(42)
random_forest_1 <- train(Diabetes_binary ~ .,
            data = train_preprocessed,
            method = "rf",
            trControl = train_control,
            tuneGrid = data.frame(mtry = 1:15)
            )

random_forest_2 <- train(Diabetes_binary ~ BMI + HighBP + HighChol + MentHlth,
      data = train_preprocessed,
            method = "rf",
            trControl = train_control,
            tuneGrid = data.frame(mtry = 1:15)
            )
      
random_forest_3 <- train(Diabetes_binary ~ BMI + HighBP + HighChol + MentHlth,
      data = train_preprocessed,
            method = "rf",
            trControl = train_control,
            tuneGrid = data.frame(mtry = 1:15)
            )

random_forest_1$results
random_forest_2$results
random_forest_3$results

```

### Ridge Regression [Andy]

```{r}

set.seed(42)
lasso <- train(Diabetes_binary ~ ., 
               data = train_preprocessed, 
               method = 'glmnet', family = 'binomial',
               trControl = train_control,
               tuneGrid = expand.grid(alpha = 1, lambda = seq(0,1,0.005)))

lasso$bestTune
plot(lasso$finalModel, "lambda")
abline(v = log(lasso$bestTune$lambda), lty = 2)
text(x=log(lasso$bestTune$lambda), y=0, labels="Lambda for\nmin CV-LogLoss", cex=0.75, adj=0.55, srt=90)
```

### Elastic Net [Heather]

```{r}

lambda <- seq(0, 3, 0.1)
alpha <- seq(0, 1, 0.1)

grid <- expand.grid(alpha = alpha, lambda = lambda)


set.seed(42)
elastic_net_1 <- train(Diabetes_binary ~ .,
            data = train_preprocessed,
            method = "glmnet",
            trControl = train_control,
            tuneGrid = grid
            )

elastic_net_2 <- train(Diabetes_binary ~ BMI + HighBP + HighChol + MentHlth,
      data = train_preprocessed,
            method = "glmnet",
            trControl = train_control,
            tuneGrid = grid
            )
      
elastic_net_3 <- train(Diabetes_binary ~ BMI + HighBP + HighChol + MentHlth,
      data = train_preprocessed,
            method = "glmnet",
            trControl = train_control,
            tuneGrid = grid
            )

elastic_net_1$results
elastic_net_2$results
elastic_net_3$results

```

## Final Model Selection

OLD CODE to be re-worked (if you want it!), copy/pasted from my HW#8

```{r compare_accuracy_on_test, eval=FALSE}
# stack model objects into a list
models <- list(tree_fit = tree_fit,
               bagtree_fit = bagtree_fit,
               rftree_fit = rftree_fit,
               boosttree_fit = boosttree_fit)

# create function to extract accuracy on test dataset
get_accuracy_on_test <- function(model){
  test_pred <- predict(model, newdata = dat_test)
  measures <- confusionMatrix(test_pred, dat_test$HeartDisease)$overall
  acc <- measures["Accuracy"]
  return( acc )
}

# use function to get test set accuracy numbers
perf <- sapply(models, get_accuracy_on_test)
perf
```

The model with the highest accuracy is the `r substr(names(which.max(perf)),1,nchar(names(which.max(perf)))-13)` model with an accuracy = `r round(max(perf),3)`.